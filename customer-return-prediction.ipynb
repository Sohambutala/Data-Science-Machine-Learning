{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Audiobooks Customer Return Prediction","metadata":{}},{"cell_type":"markdown","source":"# Information about Input data\n\nThe data in CSV is without headers and some level of pre-processing is already done. \nFor instance, the missing values of rating field are filled with the mean value (8.91)\n\nThe columns are as follows (in order)\n\n1. Customer ID\n2. Total minutes of all the audio books purchased\n3. Average minutes of all the audio books purchased\n4. Total price\n5. Average price\n6. Review ( 0 = Did not submit review, 1 = Submitted review )\n7. Review out of 10\n8. Minutes listened\n9. Completion percent\n10. Support requests made\n11. ( Last visited date - Purchase date )\n12. Customer returned to buy new audiobook (Target value) ( 1 = Customer returned, 0 = Customer did not return )","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn import preprocessing\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2021-09-16T13:57:06.186731Z","iopub.execute_input":"2021-09-16T13:57:06.187060Z","iopub.status.idle":"2021-09-16T13:57:06.192136Z","shell.execute_reply.started":"2021-09-16T13:57:06.187009Z","shell.execute_reply":"2021-09-16T13:57:06.191086Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing\n\n1. Shuffling the data to remove any time bias.\n2. Balancing the target data as 50/50 to remove the learning bias.\n3. Standardizing the inputs\n4. Shuffling again to make sure train, validation and test data set are unbiased\n\nThere are 10 input fields except ID column and 1 Target field.","metadata":{}},{"cell_type":"code","source":"# Extract\nraw_csv_data = np.loadtxt('../input/audiobook-store-customer/Audiobooks_data.csv',delimiter=',')\n\n\n# Shuffle\nshuffled_indices = np.arange(raw_csv_data.shape[0])\nnp.random.shuffle(shuffled_indices)\nshuffled_inputs = raw_csv_data[shuffled_indices]\n\n\n# Separate Input and Target\ninputs_all = shuffled_inputs[:,1:-1]\ntargets_all = shuffled_inputs[:,-1]\n\n\n# Balance data set by removing the excessive 0 targets as it will bias the learnig of model\none_targets_count = int(np.sum(targets_all))\nzero_targets_count = 0\nindices_to_remove = []\n\nfor i in range(targets_all.shape[0]):\n    if targets_all[i] == 0:\n        zero_targets_count += 1\n        if zero_targets_count > one_targets_count:\n            indices_to_remove.append(i)\n            \nbalanced_inputs_all = np.delete(inputs_all, indices_to_remove, axis=0)\nbalanced_targets_all = np.delete(targets_all, indices_to_remove, axis=0)\n\n\n# Standardize the input values\nstd_inputs = preprocessing.scale(balanced_inputs_all)\n\n\n# Shuffle again\nshuffled_indices = np.arange(inputs.shape[0])\nnp.random.shuffle(shuffled_indices)\n\ninputs = std_inputs[shuffled_indices]\ntargets = balanced_targets_all[shuffled_indices]","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-16T13:40:35.124852Z","iopub.execute_input":"2021-09-16T13:40:35.125164Z","iopub.status.idle":"2021-09-16T13:40:35.395319Z","shell.execute_reply.started":"2021-09-16T13:40:35.125125Z","shell.execute_reply":"2021-09-16T13:40:35.394496Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# Split the dataset into Train, Validation, and Test sets\n\nDataset is split in 80/10/10 parts. ","metadata":{}},{"cell_type":"code","source":"# Total samples\nsamples = inputs.shape[0]\n\n# Split count\ntrain_samples_count = int(0.8 * samples)\nvalidation_samples_count = int(0.1 * samples)\ntest_samples_count = samples - train_samples_count - validation_samples_count\n\n# Creating train set\ntrain_inputs = inputs[:train_samples_count]\ntrain_targets = targets[:train_samples_count]\n\n# Creating validation set\nvalidation_inputs = inputs[train_samples_count:train_samples_count+validation_samples_count]\nvalidation_targets = targets[train_samples_count:train_samples_count+validation_samples_count]\n\n# Creating test set\ntest_inputs = inputs[train_samples_count+validation_samples_count:]\ntest_targets = targets[train_samples_count+validation_samples_count:]\n\n# Print the number of targets that are 1s, the total number of samples, and the proportion for training, validation, and test.\nprint(\"---Train---\")\nprint(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\nprint(\"---Validation---\")\nprint(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\nprint(\"---Test---\")\nprint(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-16T13:42:46.600206Z","iopub.execute_input":"2021-09-16T13:42:46.600524Z","iopub.status.idle":"2021-09-16T13:42:46.612557Z","shell.execute_reply.started":"2021-09-16T13:42:46.600494Z","shell.execute_reply":"2021-09-16T13:42:46.611506Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"---Train---\n1799.0 3579 0.5026543727298128\n---Validation---\n212.0 447 0.4742729306487696\n---Test---\n226.0 448 0.5044642857142857\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Create *.npz for model","metadata":{}},{"cell_type":"code","source":"np.savez('./Audiobooks_data_train', inputs=train_inputs, targets=train_targets)\nnp.savez('./Audiobooks_data_validation', inputs=validation_inputs, targets=validation_targets)\nnp.savez('./Audiobooks_data_test', inputs=test_inputs, targets=test_targets)","metadata":{"execution":{"iopub.status.busy":"2021-09-16T13:41:49.559792Z","iopub.execute_input":"2021-09-16T13:41:49.560154Z","iopub.status.idle":"2021-09-16T13:41:49.571549Z","shell.execute_reply.started":"2021-09-16T13:41:49.560119Z","shell.execute_reply":"2021-09-16T13:41:49.570372Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# Load npz files","metadata":{}},{"cell_type":"code","source":"# Train set\nnpz = np.load('Audiobooks_data_train.npz')\ntrain_inputs, train_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n\n# Validation set\nnpz = np.load('Audiobooks_data_validation.npz')\nvalidation_inputs, validation_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n\n# Test set\nnpz = np.load('Audiobooks_data_test.npz')\ntest_inputs, test_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)","metadata":{"execution":{"iopub.status.busy":"2021-09-16T13:49:05.060767Z","iopub.execute_input":"2021-09-16T13:49:05.061089Z","iopub.status.idle":"2021-09-16T13:49:05.075484Z","shell.execute_reply.started":"2021-09-16T13:49:05.061059Z","shell.execute_reply":"2021-09-16T13:49:05.074529Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# Model ","metadata":{}},{"cell_type":"code","source":"input_size = 10\noutput_size = 2\n\n# Configuring the NN values. Hidden layers = 2\nhidden_layer_size = 50\nbatch_size = 120\nmax_epochs = 100\n\n# Early stopping mechanism with 2 patience level. Which means the model will continue learing until the error has been minimized \n# and cannot minimize further. The model will stop learning after 2 (patience level) such instances\nearly_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n        \nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer    \n    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n])\n\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nmodel.fit(train_inputs, \n          train_targets, \n          batch_size=batch_size, \n          epochs=max_epochs, \n          callbacks=[early_stopping], \n          validation_data=(validation_inputs, validation_targets), \n          verbose = 2 \n          )  ","metadata":{"execution":{"iopub.status.busy":"2021-09-16T14:02:36.806832Z","iopub.execute_input":"2021-09-16T14:02:36.807916Z","iopub.status.idle":"2021-09-16T14:02:39.355994Z","shell.execute_reply.started":"2021-09-16T14:02:36.807854Z","shell.execute_reply":"2021-09-16T14:02:39.354808Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Epoch 1/100\n30/30 - 1s - loss: 0.6141 - accuracy: 0.6605 - val_loss: 0.5405 - val_accuracy: 0.7539\nEpoch 2/100\n30/30 - 0s - loss: 0.5019 - accuracy: 0.7580 - val_loss: 0.4645 - val_accuracy: 0.7629\nEpoch 3/100\n30/30 - 0s - loss: 0.4477 - accuracy: 0.7714 - val_loss: 0.4212 - val_accuracy: 0.7852\nEpoch 4/100\n30/30 - 0s - loss: 0.4195 - accuracy: 0.7882 - val_loss: 0.4019 - val_accuracy: 0.8076\nEpoch 5/100\n30/30 - 0s - loss: 0.4036 - accuracy: 0.7974 - val_loss: 0.3865 - val_accuracy: 0.8098\nEpoch 6/100\n30/30 - 0s - loss: 0.3920 - accuracy: 0.8047 - val_loss: 0.3769 - val_accuracy: 0.8098\nEpoch 7/100\n30/30 - 0s - loss: 0.3865 - accuracy: 0.8055 - val_loss: 0.3708 - val_accuracy: 0.8143\nEpoch 8/100\n30/30 - 0s - loss: 0.3783 - accuracy: 0.8108 - val_loss: 0.3715 - val_accuracy: 0.8166\nEpoch 9/100\n30/30 - 0s - loss: 0.3757 - accuracy: 0.8145 - val_loss: 0.3605 - val_accuracy: 0.8166\nEpoch 10/100\n30/30 - 0s - loss: 0.3708 - accuracy: 0.8136 - val_loss: 0.3591 - val_accuracy: 0.8188\nEpoch 11/100\n30/30 - 0s - loss: 0.3678 - accuracy: 0.8117 - val_loss: 0.3577 - val_accuracy: 0.8143\nEpoch 12/100\n30/30 - 0s - loss: 0.3669 - accuracy: 0.8153 - val_loss: 0.3567 - val_accuracy: 0.8143\nEpoch 13/100\n30/30 - 0s - loss: 0.3630 - accuracy: 0.8178 - val_loss: 0.3520 - val_accuracy: 0.8233\nEpoch 14/100\n30/30 - 0s - loss: 0.3616 - accuracy: 0.8145 - val_loss: 0.3612 - val_accuracy: 0.8300\nEpoch 15/100\n30/30 - 0s - loss: 0.3612 - accuracy: 0.8184 - val_loss: 0.3503 - val_accuracy: 0.8143\nEpoch 16/100\n30/30 - 0s - loss: 0.3581 - accuracy: 0.8175 - val_loss: 0.3484 - val_accuracy: 0.8166\nEpoch 17/100\n30/30 - 0s - loss: 0.3567 - accuracy: 0.8150 - val_loss: 0.3515 - val_accuracy: 0.8233\nEpoch 18/100\n30/30 - 0s - loss: 0.3561 - accuracy: 0.8192 - val_loss: 0.3495 - val_accuracy: 0.8300\n","output_type":"stream"},{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f7c4846b390>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Testing the model","metadata":{}},{"cell_type":"code","source":"test_loss, test_accuracy = model.evaluate(test_inputs, test_targets,verbose=0)\nprint('\\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))","metadata":{"execution":{"iopub.status.busy":"2021-09-16T14:02:42.396074Z","iopub.execute_input":"2021-09-16T14:02:42.396426Z","iopub.status.idle":"2021-09-16T14:02:42.555673Z","shell.execute_reply.started":"2021-09-16T14:02:42.396393Z","shell.execute_reply":"2021-09-16T14:02:42.554376Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"\nTest loss: 0.39. Test accuracy: 80.58%\n","output_type":"stream"}]}]}